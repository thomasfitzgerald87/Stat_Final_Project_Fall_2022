---
title: "Final_Project"
author: "Marina Sanchez"
date: "2022-12-08"
output: html_document
---

# Data Preprocessing
```{r}
df <- read.csv("Table_S2_clean.csv")
head(df)
```


```{r}
summary(df)
```

```{r}
sapply(df, class)
```
```{r}
#Remove rows with NA
df = df[rowSums(is.na(df))==0,]
```


Convert to factor the categorical columns
```{r}
df$ID <- as.factor(df$ID)
df$Sex <- as.factor(df$Sex)
df$Smoking <- as.factor(df$Smoking)
df$path <- as.factor(df$path)
df$Staging <- as.factor(df$Staging)
df$EGFR_E18 <- as.factor(df$EGFR_E18)
df$BRAF <- as.factor(df$BRAF)
df$Adjuvent_Radiotherapy <- as.factor(df$Adjuvent_Radiotherapy)
df$hTERT <- as.factor(df$hTERT)
df$EGFR_E20 <- as.factor(df$EGFR_E20)
df$ALK <- as.factor(df$ALK)
df$EGFR_E19 <- as.factor(df$EGFR_E19)
df$T_of_tumor <- as.factor(df$T_of_tumor)
df$KRAS_E2 <- as.factor(df$KRAS_E2)
df$EGFR_E21 <- as.factor(df$EGFR_E21)
df$HER2 <- as.factor(df$HER2)
df$Adjuvent_Chemo <- as.factor(df$Adjuvent_Chemo)
df$Relaps <- as.factor(df$Relaps) #Our response variable
```

```{r}
#count unique values for each variable
sapply(lapply(df, unique), length)
```
Column hTERT has one unique value across the rows so we are removing this column to avoid issues when fitting the regression.
Drop also column ID

```{r}
df <- subset(df, select = -c(hTERT, ID))
```


## Train-Test Split
```{r}
#Split data into train and test
set.seed(1)

#use 70% of dataset as training set and 30% as test set
sample <- sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(0.7,0.3))
train  <- df[sample, ]
test   <- df[!sample, ]
```

# Logistic regression 
Fit an initial logistic regression with all of the predictors 
```{r}
glm.multiple <- glm(Relaps ~ .,
                    data=train,
                    family="binomial")

summary(glm.multiple)
```
We are gettin the Warning "glm.fit: fitted probabilities numerically 0 or 1 occurred". This might be happening due to extreme outliers or influential observations.

This brute force model does not look great so let's deal with outliers and start applying some model selection

## Deal with outliers
```{r}
## Influence via Cook's distance:
cooks.distance(glm.multiple)
plot(glm.multiple, which=4)
```


```{r}
#Removing outliers 
train <- train[-c(2, 8, 55), ]

#Fit Logistic regression again
glm.multiple <- glm(Relaps ~ .,
                    data=train,
                    family="binomial")

summary(glm.multiple)
```
There are some predictors that look to be significant: Sex, Age, T_of_tumor, Station_of_lymphnode_resected, Metastasis_Lymphnode, Adjuvent_Radiotherapy1 and DFS


# ---> VIF to remove colinearity
NOTE: we get the error "there are aliased coefficients in the model" which may be related to multicollinearity.

```{r}

```

# Variable selection via AIC
```{r}
step(glm.multiple)
```
Final model

```{r}
glm.multiple_final <- glm(Relaps ~  Tumor_Size + 
                                    OS + 
                                    Lymphnode_resected +
                                    Sex + 
                                    Age + 
                                    Station_of_lymphnode_resected +
                                    num_nodes_with_tumor +
                                    Adjuvent_Radiotherapy +
                                    DFS,
                        data=train,
                        family="binomial")

summary(glm.multiple_final)
```

# Test model significance using likelihood ratio test
```{r}
#significance of full model/subset of predictors

glm.null <- glm(Relaps ~ 1,
                data=train,
                family="binomial")

# Test for significance of the full model:
anova(glm.null, glm.multiple_final, test = "LRT")
```
The p-val of Full model (Model 2) is almost 0. This means that there is there is an improvement on Full model over Null. Our model is significant.

## Make predictions on the training set
```{r}
predicted.Relaps_prob = predict(glm.multiple_final, newdata=train, type="response")
```

Let's set the threshold at 0.6. Obtained probabilities over 0.6 will imply Relapse
```{r}
predicted.Relaps <- list()

for (p in predicted.Relaps_prob) {
  if (p > 0.6){
    predicted.Relaps <- append(predicted.Relaps, 1)
  }else{
    predicted.Relaps <- append(predicted.Relaps, 0)
  }
}
```

Calculate Model's accuracy
```{r}
acc <- (train$Relaps == predicted.Relaps)
accuracy <- sum(acc)/length(acc)
accuracy
```

## Make predictions on the test set
```{r}
predicted.Relaps_prob = predict(glm.multiple_final, newdata=test, type="response")
```
Let's set the threshold at 0.6. Obtained probabilities over 0.6 will imply Relapse
```{r}
predicted.Relaps <- list()

for (p in predicted.Relaps_prob) {
  if (p > 0.6){
    predicted.Relaps <- append(predicted.Relaps, 1)
  }else{
    predicted.Relaps <- append(predicted.Relaps, 0)
  }
}
```

Calculate Model's accuracy
```{r}
acc <- (test$Relaps == predicted.Relaps)
accuracy <- sum(acc)/length(acc)
accuracy
```
Our model might be a little bit overfitted since it is achieving an accuracy of 90% in our training set and drops to 80% in the test set. However, it seems to generalize pretty well.
